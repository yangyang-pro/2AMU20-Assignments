{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "task3_dummy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHkuDfWDCQL_"
      },
      "source": [
        "import os.path\n",
        "import torch\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "#!pip install datasets\n",
        "from keras.datasets import mnist\n",
        "from sklearn import datasets\n",
        "import keras\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.nn import Conv2d, BatchNorm1d, LeakyReLU, ReLU\n",
        "from copy import deepcopy\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fizb_9gCyS5",
        "outputId": "983c0a4e-cbb4-46f4-f20e-58c75d1c9fdf"
      },
      "source": [
        "#Loading MNIST dataset\n",
        "\n",
        "# get cpu or gpu device for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "\n",
        "#########################\n",
        "# Data Preparation\n",
        "###\n",
        "\n",
        "# these functions download (if needed) and load MNIST or fashion-MNIST\n",
        "# Note: sometimes the websites providing the data are down...\n",
        "#\n",
        "\n",
        "#split the dataset into 60000 training and 10000 validation samples\n",
        "#they are already normalized\n",
        "(train_x, train_labels), (test_x, test_labels) = mnist.load_data()\n",
        "#xtr = deepcopy(train_x)\n",
        "#xlbl = deepcopy(train_labels)\n",
        "#ytr = deepcopy(test_x)\n",
        "#ylbl = deepcopy(test_labels)\n",
        "#train_x, train_labels, test_x, test_labels = datasets.load_mnist()\n",
        "\n",
        "# normalize data\n",
        "#scaler = MinMaxScaler()\n",
        "#print(scaler.fit(xtr))\n",
        "#print(scaler.transform(xtr))\n",
        "# normalize data\n",
        "#train_x = datasets.normalize_min_max(train_x, 0., 1.)\n",
        "#test_x = datasets.normalize_min_max(test_x, 0., 1.)\n",
        "\n",
        "# split off a validation set (not used here, but good practice)\n",
        "valid_x = train_x[-10000:, :]\n",
        "train_x = train_x[:-10000, :]\n",
        "valid_labels = train_labels[-10000:]\n",
        "train_labels = train_labels[:-10000]\n",
        "\n",
        "# generate torch tensors\n",
        "train_x = torch.tensor(train_x).to(device)\n",
        "valid_x = torch.tensor(valid_x).to(device)\n",
        "test_x = torch.tensor(test_x).to(device)\n",
        "\n",
        "# add dummy dimension for number of color channels\n",
        "train_x = train_x.unsqueeze(1)\n",
        "valid_x = valid_x.unsqueeze(1)\n",
        "test_x = test_x.unsqueeze(1)\n",
        "\n",
        "# print some summary characteristics of the datasets\n",
        "print(\"Dataset summary:\")\n",
        "print(f\"Train: {train_x.shape}\")\n",
        "print(f\"Valid: {valid_x.shape}\")\n",
        "print(f\"Test : {test_x.shape}\")\n",
        "\n",
        "# extract number of training data points, number of color channels, and number of data features\n",
        "#train_N, train_C, train_D = train_x.shape\n",
        "train_N = train_x.shape[0]\n",
        "train_C = train_x.shape[1]\n",
        "train_D = train_x.shape[2]\n",
        "\n",
        "# compute image size from number of data features\n",
        "imgsize = int(np.sqrt(train_D))\n",
        "print(train_N,train_C,train_D)\n",
        "#----------------------------------------------------------\n",
        "dim_latent_space = 50\n",
        "batch_size = 50\n",
        "num_training_epochs = 10"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Dataset summary:\n",
            "Train: torch.Size([50000, 1, 28, 28])\n",
            "Valid: torch.Size([10000, 1, 28, 28])\n",
            "Test : torch.Size([10000, 1, 28, 28])\n",
            "50000 1 28\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuglWXVLSU2g"
      },
      "source": [
        "#####################################\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "          # input data is expected with dimensions (train_C x imgsize x imgsize)\n",
        "          # so for MNIST: 1 color channel, 28x28 in size\n",
        "                      \n",
        "          # start with three subsequent convolutional \"blocks\"\n",
        "          # every \"block\" consists of\n",
        "          # 1) a convolutional pass, to do \"local feature extraction\", with padding \n",
        "          # and stride to maintain spatial dimensions\n",
        "          # 2) ReLU activations\n",
        "          # 3) Max Pooling with 2x2 kernel, which halves the spatial size\n",
        "        self.conv1 = Conv2d(1, 64, 5,1,2)\n",
        "        self.conv2 = Conv2d(64, 128, 5,1,2)\n",
        "        self.conv3 = Conv2d(128, 256, 3,1,1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.lin = nn.Linear(256*3*3, dim_latent_space)\n",
        "        self.mu = nn.Linear(dim_latent_space, dim_latent_space)\n",
        "        self.log_sigma = nn.Linear(dim_latent_space, dim_latent_space)\n",
        "\n",
        "      #   self.encoder_module = nn.Sequential(\n",
        "      #     nn.Conv2d(1, 64, 5,1,2),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.MaxPool2d(2),\n",
        "          \n",
        "      #     # Output for MNIST is (64 x 14 x 14)\n",
        "          \n",
        "      #     nn.Conv2d(64, 128, 5,1,2),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.MaxPool2d(2),\n",
        "          \n",
        "      #     # Output for MNIST is (128 x 7 x 7)\n",
        "          \n",
        "      #     nn.Conv2d(128, 256, 3,1,1),\n",
        "      #     nn.ReLU(),\n",
        "      #     nn.MaxPool2d(2),\n",
        "          \n",
        "      #     # at this point the encoding has dimensions (256 x 3 x 3)\n",
        "      #     # intuitively, the spatial representation is down to 3x3 high-level features,\n",
        "      #     # each encoded over 256 different \"feature channels\"\n",
        "          \n",
        "      #     # flatten the encoding to a \"tensor\" (i.e. just a vector) of length 256x3x3\n",
        "      #     nn.Flatten(),\n",
        "          \n",
        "      #     # compress the encoding to a \"tensor\" (i.e. just a vector) of length dim_latent_space\n",
        "      #     # using a single (fully connected) layer with ReLU activations\n",
        "      #     nn.Linear(256*3*3, dim_latent_space),\n",
        "      #     nn.ReLU(),\n",
        "\n",
        "      #     # two more fully connected layers with ReLU activations, of dimension dim_latent_space           \n",
        "      #     nn.Linear(dim_latent_space, dim_latent_space),\n",
        "      #     nn.ReLU(),            \n",
        "      #     nn.Linear(dim_latent_space, dim_latent_space),\n",
        "      #     nn.ReLU()\n",
        "          \n",
        "      #     # output is a \"tensor\" (i.e. just a vector) of length dim_latent_space \n",
        "      # )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # the argument of this function is \"flattened\" into a single vector\n",
        "        # this reshapes the input (per row) to the desired (train_C x imgsize x imgsize)\n",
        "        # that the convolutional network can extract the local structure of\n",
        "        #print(x.size(), x[1])\n",
        "\n",
        "        x = x.resize(x.shape[0], train_C, 28, 28)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x) \n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x) \n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x) \n",
        "        x = self.flatten(x)\n",
        "        x = self.lin(x)\n",
        "        x = self.relu(x)\n",
        "        mu = self.mu(x)\n",
        "        mu = self.relu(mu)\n",
        "        log_sigma = self.log_sigma(x)\n",
        "        log_sigma = self.relu(log_sigma)\n",
        "        # compute the model outputs\n",
        "        #encoded = self.encoder_module(square_resized_x)\n",
        "        z = self.reparameterization(mu, log_sigma)\n",
        "        # return the model outputs\n",
        "        return z, mu, log_sigma\n",
        "        \n",
        "\n",
        "    def reparameterization(self, mu, log_sigma):\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        epsilon = torch.rand_like(sigma).to(device)\n",
        "        z = mu + sigma * epsilon\n",
        "        return z"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnEk2WBnDHmz"
      },
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, num_latent, num_var, var=0.05):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # generate hidden layers\n",
        "        # E.g., if num_neurons = [500, 1000, 500], then three hidden layers are generated,\n",
        "        # with 500, 1000 and 400 neurons, respectively.\n",
        "        self.var = var\n",
        "        self.num_var = num_var\n",
        "        self.num_latent = num_latent  \n",
        "\n",
        "        self.decoder_module = nn.Sequential(\n",
        "            # input data is expected with dimensions (dim_latent_space x 1 x 1)\n",
        "            \n",
        "            # start with three subsequent transposed convolutional (\"de-convolutional\") \"blocks\"\n",
        "            # every \"block\" consists of\n",
        "            # 1) a deconvolutional pass, to do \"local feature upsampling\"\n",
        "            # 2) a batch normalisation to standardize outputs\n",
        "            # 3) ReLU activations\n",
        "            \n",
        "            # first transposed convolutional pass (\"de-convolution\")\n",
        "            # this turns the representation into a tensor of dimensions (256 x 3 x 3)\n",
        "            nn.ConvTranspose2d(dim_latent_space,  256, 5, 2, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # second transposed convolutional pass (\"de-convolution\")\n",
        "            # this turns the representation into a tensor of dimensions (128 x 7 x 7)\n",
        "            nn.ConvTranspose2d(256, 128, 5, 2, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # third transposed convolutional pass (\"de-convolution\")\n",
        "            # this turns the representation into a tensor of dimensions (64 x 15 x 15)\n",
        "            nn.ConvTranspose2d(128, 64, 5, 2, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "            # at this point, do a final transposed convolutional pass (\"de-convolution\")\n",
        "            # this turns the representation into a tensor of dimensions (train_C x 31 x 31)\n",
        "            nn.ConvTranspose2d(64, train_C, 5, 2, 1),\n",
        "\n",
        "            # end with sigmoid activations to ensure the outputs are in the range [0,1]\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # turn the \"flattened\" argument into shape (ndata x dim_latent_space x 1 x 1)\n",
        "        # by adding two \"dummy\" dimensions at the end\n",
        "        x=x.unsqueeze(-1).unsqueeze(-1)\n",
        "        \n",
        "        # compute the model outputs\n",
        "        preds = self.decoder_module(x)\n",
        "        \n",
        "        # the decoder output is an image of size 31x31\n",
        "        # select a center portion of the correct size\n",
        "        preds = preds[:, :, 1:29, 1:29 ]\n",
        "        \n",
        "        # return the decoded image\n",
        "        return preds\n",
        "        \n",
        "    def sample(self, N, convert_to_numpy=False, suppress_noise=True):\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(N, self.num_latent, device=device)\n",
        "            mu = self.forward(z)\n",
        "            x = mu\n",
        "            # the conditional VAE distribution is isotropic Gaussian, hence we just add noise when sampling it\n",
        "            # for images, one might want to suppress this\n",
        "            if not suppress_noise:\n",
        "                x += np.sqrt(self.var) * torch.randn(N, self.num_var, device=device)\n",
        "\n",
        "        if convert_to_numpy:\n",
        "            z = z.cpu().numpy()\n",
        "            x = x.cpu().numpy()\n",
        "        return x, z"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyyeRynVDXkz",
        "outputId": "ad6045c0-8477-4cc2-d06c-1dd1d224838a"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# training params\n",
        "# MNIST has 784 pixels\n",
        "# we are using a fixed variance for the decoder\n",
        "# the variance can be also made an output of the decoder, see lecture slides,\n",
        "# but training becomes trickier and somewhat \"brittle\"\n",
        "\n",
        "var_x = 0.05\n",
        "num_latent = 50\n",
        "# num_hidden = [1000, 1000]\n",
        "# batch_size = 50\n",
        "# num_epochs = 2\n",
        "\n",
        "decoder = Decoder(num_latent,var_x).to(device)\n",
        "encoder = Encoder().to(device)\n",
        "print(decoder)\n",
        "print(encoder)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder(\n",
            "  (decoder_module): Sequential(\n",
            "    (0): ConvTranspose2d(50, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
            "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
            "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU()\n",
            "    (6): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
            "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU()\n",
            "    (9): ConvTranspose2d(64, 1, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n",
            "Encoder(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (lin): Linear(in_features=2304, out_features=50, bias=True)\n",
            "  (mu): Linear(in_features=50, out_features=50, bias=True)\n",
            "  (log_sigma): Linear(in_features=50, out_features=50, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "2gv0htk-ElZP",
        "outputId": "aa9190a7-4df4-484a-e666-669e5f400c50"
      },
      "source": [
        "encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=1e-4)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(),lr=1e-4)\n",
        "\n",
        "ELBO_history = []\n",
        "for epoch in range(num_training_epochs):\n",
        "    # make batches of training indices\n",
        "    shuffled_idx = torch.randperm(train_x.shape[0])\n",
        "    idx_batches = shuffled_idx.split(batch_size)\n",
        "\n",
        "    sum_neg_ELBO = 0.0\n",
        "    for batch_count, idx in enumerate(idx_batches):\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        batch_x = train_x[idx, :]\n",
        "        #batch_x = batch_x.float()\n",
        "        # batch_mu_z: batch_size, num_latent\n",
        "        # batch_var_z: batch_size, num_latent\n",
        "        \n",
        "        batch_x = batch_x.float()\n",
        "        #print(len(batch_x))\n",
        "\n",
        "        batch_mu_z, batch_var_z, batch_z = encoder(batch_x)\n",
        "        \n",
        "        # sample z, using the \"reparametrization trick\"\n",
        "        #batch_z = batch_mu_z + torch.sqrt(batch_var_z) * torch.randn(batch_var_z.shape, device=device)\n",
        "\n",
        "        # mu_x: batch_size, D\n",
        "        mu_x = decoder(batch_z)\n",
        "\n",
        "        # squared distances between mu_x and batch_x\n",
        "        d2 = (mu_x - batch_x) ** 2\n",
        "        # Gaussian likelihood: 1/sqrt(2*pi*var) exp(-0.5 * (mu-x)**2 / var)\n",
        "        # Thus, log-likelihood = -0.5 * ( log(2*pi*var) + (mu-x)**2 / var )\n",
        "        log_p = -0.5 * torch.sum(np.log(decoder.var * 2 * np.pi) + d2 / decoder.var)\n",
        "        KL = -0.5 * torch.sum(1 + torch.log(batch_var_z) - batch_mu_z**2 - batch_var_z)\n",
        "\n",
        "        # we want to maximize the ELBO, hence minimize the negative ELBO\n",
        "        negative_ELBO = -log_p + KL\n",
        "        negative_ELBO.backward()\n",
        "        # update all model parameters\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        sum_neg_ELBO += negative_ELBO\n",
        "\n",
        "    mean_neg_ELBO = sum_neg_ELBO / train_x.shape[0]\n",
        "    print('epoch {}   mean negative ELBO = {}'.format(epoch, mean_neg_ELBO))\n",
        "    ELBO_history.append(mean_neg_ELBO)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        with torch.no_grad():\n",
        "            # sample from the VAE\n",
        "            x, z = decoder.sample(5)\n",
        "            train_x = train_x.float()\n",
        "            # encode some samples\n",
        "            bat_z,mu_z, var_z = encoder(train_x[0:5, :])\n",
        "            z_encoded = mu_z + torch.sqrt(var_z) * torch.randn(5, num_latent, device=device)\n",
        "\n",
        "            # decode the samples\n",
        "            x_decoded = decoder(z_encoded)\n",
        "\n",
        "            # save images\n",
        "            plot_img = np.stack((train_x[0:5, :].detach().cpu().numpy(),\n",
        "                                 x_decoded.detach().cpu().numpy(),\n",
        "                                 x.detach().cpu().numpy()))\n",
        "            plot_img = np.reshape(plot_img, (15, 28, 28))\n",
        "            file_name = os.path.join('img_vae', 'samples_{}.png'.format(epoch))\n",
        "            #datasets.save_image_stack(plot_img, 3, 5, file_name, margin = 3)\n",
        "\n",
        "        plt.figure(1)\n",
        "        plt.clf()\n",
        "        plt.plot(ELBO_history)\n",
        "        plt.savefig(os.path.join('img_vae', 'elbo.png'))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/tensor.py:474: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0   mean negative ELBO = inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-9919ae62a608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mELBO_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img_vae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'elbo.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# need this if 'transparent=True' to reset colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2126\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2127\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m                 _png.write_png(renderer._renderer, fh, self.figure.dpi,\n\u001b[1;32m    537\u001b[0m                                metadata={**default_metadata, **metadata})\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mopen_file_cm\u001b[0;34m(path_or_file, mode, encoding)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34mr\"\"\"Pass through file objects and context-manage `.PathLike`\\s.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_filehandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mto_filehandle\u001b[0;34m(fname, flag, return_opened, encoding)\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'seek'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'img_vae/elbo.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}