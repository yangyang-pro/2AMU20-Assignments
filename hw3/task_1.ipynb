{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "task3_dummy.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "OHkuDfWDCQL_"
   },
   "source": [
    "%matplotlib inline\n",
    "import os.path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fizb_9gCyS5",
    "outputId": "1cc36cbd-620e-43a2-c159-556333324fcb"
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "trans = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0,), (1.0,))])\n",
    "train_metadata = torchvision.datasets.MNIST(root='./data', train=True, transform=trans, download=True)\n",
    "test_metadata = torchvision.datasets.MNIST(root='./data', train=False, transform=trans, download=True)\n",
    "\n",
    "train_data = train_metadata.data[:50000] / 255\n",
    "val_data = train_metadata.data[-10000:] / 255\n",
    "test_data = test_metadata.data / 255\n",
    "train_labels = train_metadata.targets[:50000]\n",
    "val_labels = train_metadata.targets[-10000:]\n",
    "test_labels = test_metadata.targets\n"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XnEk2WBnDHmz"
   },
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        # generate hidden layers\n",
    "        # E.g., if num_neurons = [500, 1000, 500], then three hidden layers are generated,\n",
    "        # with 500, 1000 and 400 neurons, respectively.\n",
    "        self.decoder_module = nn.Sequential(\n",
    "            \n",
    "            # input data is expected with dimensions (dim_latent_space x 1 x 1)\n",
    "            \n",
    "            # start with three subsequent transposed convolutional (\"de-convolutional\") \"blocks\"\n",
    "            # every \"block\" consists of\n",
    "            # 1) a deconvolutional pass, to do \"local feature upsampling\"\n",
    "            # 2) a batch normalisation to standardize outputs\n",
    "            # 3) ReLU activations\n",
    "            \n",
    "            # first transposed convolutional pass (\"de-convolution\")\n",
    "            # this turns the representation into a tensor of dimensions (256 x 3 x 3)\n",
    "            nn.ConvTranspose2d(dim_latent_space,  256, 5, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # second transposed convolutional pass (\"de-convolution\")\n",
    "            # this turns the representation into a tensor of dimensions (128 x 7 x 7)\n",
    "            nn.ConvTranspose2d(256, 128, 5, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # third transposed convolutional pass (\"de-convolution\")\n",
    "            # this turns the representation into a tensor of dimensions (64 x 15 x 15)\n",
    "            nn.ConvTranspose2d(128, 64, 5, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # at this point, do a final transposed convolutional pass (\"de-convolution\")\n",
    "            # this turns the representation into a tensor of dimensions (train_C x 31 x 31)\n",
    "            nn.ConvTranspose2d(64, train_C, 5, 2, 1),\n",
    "\n",
    "            # end with sigmoid activations to ensure the outputs are in the range [0,1]\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # turn the \"flattened\" argument into shape (ndata x dim_latent_space x 1 x 1)\n",
    "        # by adding two \"dummy\" dimensions at the end\n",
    "        x=x.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # compute the model outputs\n",
    "        preds = self.decoder_module(x)\n",
    "        \n",
    "        # the decoder output is an image of size 31x31\n",
    "        # select a center portion of the correct size\n",
    "        preds = preds[:, :, 1:29, 1:29 ]\n",
    "        \n",
    "        # return the decoded image\n",
    "        return preds\n",
    "        \n",
    "    # def sample(self, N, convert_to_numpy=False, suppress_noise=True):\n",
    "    #     with torch.no_grad():\n",
    "    #         z = torch.randn(N, self.num_latent, device=device)\n",
    "    #         mu = self.forward(z)\n",
    "    #         x = mu\n",
    "    #         # the conditional VAE distribution is isotropic Gaussian, hence we just add noise when sampling it\n",
    "    #         # for images, one might want to suppress this\n",
    "    #         if not suppress_noise:\n",
    "    #             x += np.sqrt(self.var) * torch.randn(N, self.num_var, device=device)\n",
    "\n",
    "    #     if convert_to_numpy:\n",
    "    #         z = z.cpu().numpy()\n",
    "    #         x = x.cpu().numpy()\n",
    "    #     return x, z"
   ],
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3ieHdPA6DU27"
   },
   "source": [
    "#####################################\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        \n",
    "        self.encoder_module = nn.Sequential(\n",
    "\n",
    "          # input data is expected with dimensions (train_C x imgsize x imgsize)\n",
    "          # so for MNIST: 1 color channel, 28x28 in size\n",
    "                      \n",
    "          # start with three subsequent convolutional \"blocks\"\n",
    "          # every \"block\" consists of\n",
    "          # 1) a convolutional pass, to do \"local feature extraction\", with padding \n",
    "          # and stride to maintain spatial dimensions\n",
    "          # 2) ReLU activations\n",
    "          # 3) Max Pooling with 2x2 kernel, which halves the spatial size\n",
    "          nn.Conv2d(1, 64, 5,1,2),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2),\n",
    "          \n",
    "          # Output for MNIST is (64 x 14 x 14)\n",
    "          \n",
    "          nn.Conv2d(64, 128, 5,1,2),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2),\n",
    "          \n",
    "          # Output for MNIST is (128 x 7 x 7)\n",
    "          \n",
    "          nn.Conv2d(128, 256, 3,1,1),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2),\n",
    "          \n",
    "          # at this point the encoding has dimensions (256 x 3 x 3)\n",
    "          # intuitively, the spatial representation is down to 3x3 high-level features,\n",
    "          # each encoded over 256 different \"feature channels\"\n",
    "          \n",
    "          # flatten the encoding to a \"tensor\" (i.e. just a vector) of length 256x3x3\n",
    "          nn.Flatten(),\n",
    "          \n",
    "          # compress the encoding to a \"tensor\" (i.e. just a vector) of length dim_latent_space\n",
    "          # using a single (fully connected) layer with ReLU activations\n",
    "          nn.Linear(256*3*3, dim_latent_space),\n",
    "          nn.ReLU(),\n",
    "\n",
    "          # two more fully connected layers with ReLU activations, of dimension dim_latent_space           \n",
    "          nn.Linear(dim_latent_space, dim_latent_space),\n",
    "          nn.ReLU(),            \n",
    "          nn.Linear(dim_latent_space, dim_latent_space),\n",
    "          nn.ReLU()\n",
    "          \n",
    "          # output is a \"tensor\" (i.e. just a vector) of length dim_latent_space \n",
    "      )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # the argument of this function is \"flattened\" into a single vector\n",
    "        # this reshapes the input (per row) to the desired (train_C x imgsize x imgsize)\n",
    "        # that the convolutional network can extract the local structure of\n",
    "        square_resized_x = x.resize(x.shape[0], train_C, 28, 28)\n",
    "        \n",
    "        # compute the model outputs\n",
    "        encoded = self.encoder_module(square_resized_x)\n",
    "        \n",
    "        # return the model outputs\n",
    "        return encoded"
   ],
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyyeRynVDXkz",
    "outputId": "3a9213e8-de79-40ee-e082-e851a6e6e146"
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# training params\n",
    "# MNIST has 784 pixels\n",
    "# we are using a fixed variance for the decoder\n",
    "# the variance can be also made an output of the decoder, see lecture slides,\n",
    "# but training becomes trickier and somewhat \"brittle\"\n",
    "\n",
    "# var_x = 0.05\n",
    "# num_latent = 50\n",
    "# num_hidden = [1000, 1000]\n",
    "# batch_size = 50\n",
    "# num_epochs = 2\n",
    "\n",
    "decoder = Decoder().to(device)\n",
    "encoder = Encoder().to(device)\n",
    "print(decoder)\n",
    "print(encoder)"
   ],
   "execution_count": 60,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (decoder_module): Sequential(\n",
      "    (0): ConvTranspose2d(50, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): ConvTranspose2d(64, 1, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Encoder(\n",
      "  (encoder_module): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Flatten(start_dim=1, end_dim=-1)\n",
      "    (10): Linear(in_features=2304, out_features=50, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (15): ReLU()\n",
      "  )\n",
      ")\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "2gv0htk-ElZP",
    "outputId": "d07ff97f-d65d-47f5-d1e8-e5583e572804"
   },
   "source": [
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(),lr=1e-4)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(),lr=1e-4)\n",
    "\n",
    "ELBO_history = []\n",
    "for epoch in range(num_training_epochs):\n",
    "    # make batches of training indices\n",
    "    shuffled_idx = torch.randperm(train_x.shape[0])\n",
    "    idx_batches = shuffled_idx.split(batch_size)\n",
    "\n",
    "    sum_neg_ELBO = 0.0\n",
    "    for batch_count, idx in enumerate(idx_batches):\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        batch_x = train_x[idx, :]\n",
    "        #batch_x = batch_x.float()\n",
    "        # batch_mu_z: batch_size, num_latent\n",
    "        # batch_var_z: batch_size, num_latent\n",
    "        \n",
    "        \n",
    "        batch_mu_z, batch_var_z = encoder(batch_x)\n",
    "\n",
    "        # sample z, using the \"reparametrization trick\"\n",
    "        batch_z = batch_mu_z + torch.sqrt(batch_var_z) * torch.randn(batch_var_z.shape, device=device)\n",
    "\n",
    "        # mu_x: batch_size, D\n",
    "        mu_x = decoder(batch_z)\n",
    "\n",
    "        # squared distances between mu_x and batch_x\n",
    "        d2 = (mu_x - batch_x) ** 2\n",
    "        # Gaussian likelihood: 1/sqrt(2*pi*var) exp(-0.5 * (mu-x)**2 / var)\n",
    "        # Thus, log-likelihood = -0.5 * ( log(2*pi*var) + (mu-x)**2 / var )\n",
    "        log_p = -0.5 * torch.sum(np.log(decoder.var * 2 * np.pi) + d2 / decoder.var)\n",
    "        KL = -0.5 * torch.sum(1 + torch.log(batch_var_z) - batch_mu_z**2 - batch_var_z)\n",
    "\n",
    "        # we want to maximize the ELBO, hence minimize the negative ELBO\n",
    "        negative_ELBO = -log_p + KL\n",
    "        negative_ELBO.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        sum_neg_ELBO += negative_ELBO\n",
    "\n",
    "    mean_neg_ELBO = sum_neg_ELBO / train_x.shape[0]\n",
    "    print('epoch {}   mean negative ELBO = {}'.format(epoch, mean_neg_ELBO))\n",
    "    ELBO_history.append(mean_neg_ELBO)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad():\n",
    "            # sample from the VAE\n",
    "            x, z = decoder.sample(5)\n",
    "\n",
    "            # encode some samples\n",
    "            mu_z, var_z = encoder(train_x[0:5, :])\n",
    "            z_encoded = mu_z + torch.sqrt(var_z) * torch.randn(5, num_latent, device=device)\n",
    "\n",
    "            # decode the samples\n",
    "            x_decoded = decoder(z_encoded)\n",
    "\n",
    "            # save images\n",
    "            plot_img = np.stack((train_x[0:5, :].detach().cpu().numpy(),\n",
    "                                 x_decoded.detach().cpu().numpy(),\n",
    "                                 x.detach().cpu().numpy()))\n",
    "            plot_img = np.reshape(plot_img, (15, 28, 28))\n",
    "            file_name = os.path.join('img_vae', 'samples_{}.png'.format(epoch))\n",
    "            datasets.save_image_stack(plot_img, 3, 5, file_name, margin = 3)\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.clf()\n",
    "        plt.plot(ELBO_history)\n",
    "        plt.savefig(os.path.join('img_vae', 'elbo.png'))\n"
   ],
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/tensor.py:474: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-61-9ff7bde1c8b1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 20\u001B[0;31m         \u001B[0mbatch_mu_z\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbatch_var_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_x\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m         \u001B[0;31m# sample z, using the \"reparametrization trick\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-59-499193f32646>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;31m# compute the model outputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m         \u001B[0mencoded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoder_module\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msquare_resized_x\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0;31m# return the model outputs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    117\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 119\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    120\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    398\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 399\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    400\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    394\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[1;32m    395\u001B[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001B[0;32m--> 396\u001B[0;31m                         self.padding, self.dilation, self.groups)\n\u001B[0m\u001B[1;32m    397\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    398\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected scalar type Byte but found Float"
     ]
    }
   ]
  }
 ]
}